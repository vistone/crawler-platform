package main

import (
	"context"
	"crypto/tls"
	"flag"
	"fmt"
	"log"
	"os"
	"os/signal"
	"sort"
	"strings"
	"sync"
	"sync/atomic"
	"syscall"
	"time"

	"crawler-platform/cmd/grpcserver/tasksmanager"
	"crawler-platform/logger"

	"google.golang.org/grpc"
	"google.golang.org/grpc/credentials"
	"google.golang.org/grpc/credentials/insecure"
)

func main() {
	// è§£æå‘½ä»¤è¡Œå‚æ•°ï¼ˆé…ç½®æ–‡ä»¶è·¯å¾„å’Œå…¶ä»–è¦†ç›–é€‰é¡¹ï¼‰
	configPath := flag.String("config", "", "é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤: ./cmd/grpcclient/config.toml æˆ– ./config.tomlï¼‰")
	protocolType := flag.String("protocol", "", "åè®®ç±»å‹: grpc, tuicï¼ˆè¦†ç›–é…ç½®æ–‡ä»¶ï¼‰")
	serverAddr := flag.String("server", "", "æœåŠ¡å™¨åœ°å€ï¼ˆè¦†ç›–é…ç½®æ–‡ä»¶ï¼‰")
	clientName := flag.String("name", "", "å®¢æˆ·ç«¯åç§°ï¼ˆè¦†ç›–é…ç½®æ–‡ä»¶ï¼‰")
	certsDir := flag.String("certs", "", "è¯ä¹¦ç›®å½•è·¯å¾„ï¼ˆè¦†ç›–é…ç½®æ–‡ä»¶ï¼‰")
	insecureMode := flag.Bool("insecure", false, "ä½¿ç”¨éåŠ å¯†è¿æ¥ï¼ˆè¦†ç›–é…ç½®æ–‡ä»¶ï¼Œä»… gRPCï¼‰")
	tuicUUID := flag.String("uuid", "", "TUIC UUIDï¼ˆè¦†ç›–é…ç½®æ–‡ä»¶ï¼Œç”¨äºçœŸæ­£çš„ TUIC åè®®ï¼‰")
	tuicPassword := flag.String("password", "", "TUIC å¯†ç ï¼ˆè¦†ç›–é…ç½®æ–‡ä»¶ï¼Œç”¨äºçœŸæ­£çš„ TUIC åè®®ï¼‰")
	tileKey := flag.String("tilekey", "", "ç“¦ç‰‡é”®ï¼ˆè¦†ç›–é…ç½®æ–‡ä»¶ï¼‰")
	epoch := flag.Int("epoch", 0, "ä¸»ç‰ˆæœ¬å·ï¼ˆè¦†ç›–é…ç½®æ–‡ä»¶ï¼Œ0 è¡¨ç¤ºä½¿ç”¨é…ç½®æ–‡ä»¶çš„å€¼ï¼‰")
	taskType := flag.String("tasktype", "", "ä»»åŠ¡ç±»å‹ï¼ˆè¦†ç›–é…ç½®æ–‡ä»¶ï¼‰")
	repeatCount := flag.Int("repeat", 0, "é‡å¤è¯·æ±‚æ¬¡æ•°ï¼ˆè¦†ç›–é…ç½®æ–‡ä»¶ï¼Œ0 è¡¨ç¤ºä½¿ç”¨é…ç½®æ–‡ä»¶çš„å€¼ï¼‰")
	concurrency := flag.Int("concurrency", 0, "å¹¶å‘è¯·æ±‚æ•°é‡ï¼ˆè¦†ç›–é…ç½®æ–‡ä»¶ï¼Œ0 è¡¨ç¤ºä½¿ç”¨é…ç½®æ–‡ä»¶çš„å€¼ï¼‰")
	flag.Parse()

	// åŠ è½½é…ç½®æ–‡ä»¶
	cfg, err := LoadConfig(*configPath)
	if err != nil {
		log.Fatalf("åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: %v", err)
	}

	// éªŒè¯é…ç½®
	if err := cfg.Validate(); err != nil {
		log.Fatalf("é…ç½®éªŒè¯å¤±è´¥: %v", err)
	}

	// å‘½ä»¤è¡Œå‚æ•°è¦†ç›–é…ç½®æ–‡ä»¶ï¼ˆå¦‚æœæä¾›äº†ï¼‰
	if *protocolType != "" {
		cfg.Protocol.Type = *protocolType
	}
	if *serverAddr != "" {
		cfg.Server.Address = *serverAddr
	}
	if *clientName != "" {
		cfg.Client.Name = *clientName
	}
	if *certsDir != "" {
		cfg.Server.CertsDir = *certsDir
	}
	if *insecureMode {
		cfg.Server.Insecure = true
	}
	if *tuicUUID != "" {
		cfg.Server.UUID = *tuicUUID
	}
	if *tuicPassword != "" {
		cfg.Server.Password = *tuicPassword
	}
	if *tileKey != "" {
		cfg.Task.TileKey = *tileKey
	}
	if *epoch > 0 {
		cfg.Task.Epoch = int32(*epoch)
	}
	if *taskType != "" {
		cfg.Task.TaskType = *taskType
	}
	if *repeatCount > 0 {
		cfg.Task.RepeatCount = *repeatCount
	}
	if *concurrency > 0 {
		cfg.Task.Concurrency = *concurrency
	}

	// åˆå§‹åŒ–æ—¥å¿—è®°å½•å™¨ï¼ˆæ ¹æ®é…ç½®æ–‡ä»¶ï¼‰
	logger.InitGlobalLogger(logger.NewConsoleLogger(
		cfg.Logger.EnableDebug,
		cfg.Logger.EnableInfo,
		cfg.Logger.EnableWarn,
		cfg.Logger.EnableError,
	))

	if *configPath != "" {
		log.Printf("å·²åŠ è½½é…ç½®æ–‡ä»¶: %s", *configPath)
	} else {
		log.Println("ä½¿ç”¨é»˜è®¤é…ç½®ï¼ˆå¯é€šè¿‡ -config æŒ‡å®šé…ç½®æ–‡ä»¶ï¼‰")
	}

	ctx := context.Background()

	var client tasksmanager.TasksManagerClient
	var conn *grpc.ClientConn
	var dualClient *DualProtocolClient

	// æ ¹æ®åè®®ç±»å‹åˆ›å»ºå®¢æˆ·ç«¯
	// å¦‚æœé…ç½®äº†èŠ‚ç‚¹åˆ—è¡¨ï¼Œè·³è¿‡ç›´æ¥è¿æ¥ï¼Œåªä½¿ç”¨èŠ‚ç‚¹æ± 
	hasNodeList := len(cfg.Server.Nodes) > 0

	switch cfg.Protocol.Type {
	case "both":
		// åŒåè®®æ¨¡å¼ï¼šå¦‚æœé…ç½®äº†èŠ‚ç‚¹åˆ—è¡¨ï¼Œä¸åˆ›å»ºç›´æ¥è¿æ¥ï¼Œåªä½¿ç”¨èŠ‚ç‚¹æ± 
		if hasNodeList {
			log.Println("ğŸ“¡ æ£€æµ‹åˆ°èŠ‚ç‚¹åˆ—è¡¨é…ç½®ï¼Œå°†è·³è¿‡ç›´æ¥è¿æ¥ï¼Œä»…ä½¿ç”¨èŠ‚ç‚¹æ± ")
			// åˆ›å»ºä¸€ä¸ªå ä½å®¢æˆ·ç«¯ï¼ˆå®é™…ä¸Šä¸ä¼šä½¿ç”¨ï¼‰
			client = nil
		} else {
			// åŒåè®®æ¨¡å¼ï¼šä¼˜å…ˆä½¿ç”¨ TUICï¼Œå¤±è´¥æ—¶è‡ªåŠ¨åˆ‡æ¢åˆ° gRPC
			var err error
			dualClient, err = NewDualProtocolClient(cfg)
			if err != nil {
				log.Fatalf("åˆ›å»ºåŒåè®®å®¢æˆ·ç«¯å¤±è´¥: %v", err)
			}
			defer dualClient.Close()
			client = dualClient
		}
	case "tuic":
		// ä»…ä½¿ç”¨ TUIC å®¢æˆ·ç«¯
		var tuicClient TUICClient
		if cfg.Server.UUID != "" {
			singBoxClient, err := NewSingBoxTUICClient(cfg.Server.TUICAddress, cfg.Server.UUID, cfg.Server.Password)
			if err != nil {
				log.Printf("åˆ›å»º sing-box TUIC å®¢æˆ·ç«¯å¤±è´¥: %vï¼Œå°†ä½¿ç”¨ HTTP æ¥å£æ¨¡å¼", err)
				tuicClient = NewHTTPTUICClient(cfg.Server.TUICAddress)
				log.Printf("å·²åˆ›å»º TUIC å®¢æˆ·ç«¯ï¼ˆHTTP æ¥å£æ¨¡å¼ï¼‰ï¼Œè¿æ¥åˆ°: %s", cfg.Server.TUICAddress)
			} else {
				tuicClient = singBoxClient
				log.Printf("å·²åˆ›å»º sing-box TUIC å®¢æˆ·ç«¯ï¼Œè¿æ¥åˆ°: %s (UUID: %s)", cfg.Server.TUICAddress, cfg.Server.UUID)
			}
		} else {
			tuicClient = NewHTTPTUICClient(cfg.Server.TUICAddress)
			log.Printf("å·²åˆ›å»º TUIC å®¢æˆ·ç«¯ï¼ˆHTTP æ¥å£æ¨¡å¼ï¼‰ï¼Œè¿æ¥åˆ°: %s", cfg.Server.TUICAddress)
		}
		client = newTUICClientAdapter(tuicClient)
	case "grpc":
		// ä»…ä½¿ç”¨ gRPC å®¢æˆ·ç«¯
		var transportCreds credentials.TransportCredentials
		if cfg.Server.Insecure {
			transportCreds = insecure.NewCredentials()
			log.Printf("ä½¿ç”¨éåŠ å¯†è¿æ¥ï¼ˆinsecure æ¨¡å¼ï¼‰")
		} else if cfg.Server.CertsDir != "" {
			tlsConfig, err := LoadTLSConfigFromCertsDir(cfg.Server.CertsDir)
			if err == nil {
				transportCreds = credentials.NewTLS(tlsConfig)
				log.Printf("å·²åŠ è½½ TLS è¯ä¹¦ï¼Œè¯ä¹¦ç›®å½•: %s", cfg.Server.CertsDir)
			} else {
				transportCreds = insecure.NewCredentials()
				log.Printf("åŠ è½½ TLS è¯ä¹¦å¤±è´¥ï¼Œä½¿ç”¨éåŠ å¯†è¿æ¥: %v", err)
			}
		} else {
			transportCreds = insecure.NewCredentials()
			log.Printf("æœªæŒ‡å®šè¯ä¹¦ç›®å½•ï¼Œä½¿ç”¨éåŠ å¯†è¿æ¥")
		}

		var err error
		conn, err = grpc.NewClient(cfg.Server.GRPCAddress, grpc.WithTransportCredentials(transportCreds))
		if err != nil {
			log.Fatalf("è¿æ¥æœåŠ¡å™¨å¤±è´¥: %v", err)
		}
		defer func() {
			if conn != nil {
				conn.Close()
			}
		}()

		client = tasksmanager.NewTasksManagerClient(conn)
		log.Printf("å·²åˆ›å»º gRPC å®¢æˆ·ç«¯ï¼Œè¿æ¥åˆ°: %s", cfg.Server.GRPCAddress)
	default:
		log.Fatalf("ä¸æ”¯æŒçš„åè®®ç±»å‹: %s (æ”¯æŒ: grpc, tuic, both)", cfg.Protocol.Type)
	}

	// æäº¤çœŸå®æ•°æ®è¯·æ±‚
	log.Printf("=== æäº¤çœŸå®æ•°æ®è¯·æ±‚ ===")
	log.Printf("ä»»åŠ¡ç±»å‹: %s, TileKey: %s, epoch: %d, é‡å¤æ¬¡æ•°: %d, å¹¶å‘æ•°: %d",
		cfg.Task.TaskType, cfg.Task.TileKey, cfg.Task.Epoch, cfg.Task.RepeatCount, cfg.Task.Concurrency)

	// å¦‚æœæ˜¯ gRPC æ¨¡å¼æˆ– both æ¨¡å¼ï¼Œå…ˆæ³¨å†Œå®¢æˆ·ç«¯å¹¶åˆå§‹åŒ–èŠ‚ç‚¹æ± 
	var nodePool *NodePool
	var clientID string
	if cfg.Protocol.Type == "grpc" || cfg.Protocol.Type == "both" {
		log.Println("\n=== å®¢æˆ·ç«¯æ³¨å†Œå’ŒèŠ‚ç‚¹æ± åˆå§‹åŒ– ===")

		// åˆ›å»ºèŠ‚ç‚¹æ± ç®¡ç†å™¨
		var nodePoolTLSConfig *tls.Config
		if cfg.Server.CertsDir != "" {
			if config, err := LoadTLSConfigFromCertsDir(cfg.Server.CertsDir); err == nil {
				nodePoolTLSConfig = config
			}
		}
		nodePool = NewNodePool(nodePoolTLSConfig)
		defer nodePool.Close()

		// å¦‚æœé…ç½®äº†èŠ‚ç‚¹åˆ—è¡¨ï¼Œç›´æ¥è¿æ¥è¿™äº›èŠ‚ç‚¹
		if len(cfg.Server.Nodes) > 0 {
			log.Printf("ğŸ“¡ ä½¿ç”¨é…ç½®çš„èŠ‚ç‚¹åˆ—è¡¨ï¼Œå…± %d ä¸ªèŠ‚ç‚¹", len(cfg.Server.Nodes))
			log.Printf("   å°†å°è¯•è¿æ¥åˆ°æ¯ä¸ªèŠ‚ç‚¹çš„ 50051ï¼ˆgRPCï¼‰å’Œ 8443ï¼ˆTUICï¼‰ç«¯å£")

			// è¿æ¥åˆ°é…ç½®çš„èŠ‚ç‚¹åˆ—è¡¨
			successCount := 0
			for _, nodeIP := range cfg.Server.Nodes {
				if nodeIP == "" {
					continue
				}
				log.Printf("ğŸ”— æ­£åœ¨è¿æ¥èŠ‚ç‚¹: %s", nodeIP)

				// åˆ›å»ºä¸´æ—¶èŠ‚ç‚¹ä¿¡æ¯ï¼ˆä½¿ç”¨ IP:Port ä½œä¸º UUIDï¼‰
				nodeUUID := fmt.Sprintf("%s:50051", nodeIP)
				nodeInfo := &tasksmanager.GrpcServerNodeInfo{
					NodeUuid: nodeUUID,
					NodeIp:   nodeIP,
					NodePort: "50051",
				}

				// å°è¯•æ·»åŠ åˆ°èŠ‚ç‚¹æ± ï¼ˆä¼šè‡ªåŠ¨è¿æ¥ gRPC å’Œè·å– TUIC é…ç½®ï¼‰
				if err := nodePool.AddNode(nodeInfo); err != nil {
					log.Printf("âŒ è¿æ¥èŠ‚ç‚¹ %s å¤±è´¥: %v", nodeIP, err)
				} else {
					successCount++
					log.Printf("âœ… èŠ‚ç‚¹ %s è¿æ¥æˆåŠŸ", nodeIP)
				}
			}

			log.Printf("ğŸ“Š èŠ‚ç‚¹è¿æ¥å®Œæˆ: æˆåŠŸ=%d/%d, èŠ‚ç‚¹æ± æ€»æ•°=%d, å¥åº·=%d",
				successCount, len(cfg.Server.Nodes), nodePool.GetNodeCount(), nodePool.GetHealthyNodeCount())

			// æ£€æŸ¥èŠ‚ç‚¹æ± æ˜¯å¦æœ‰å¯ç”¨èŠ‚ç‚¹
			if nodePool.GetHealthyNodeCount() == 0 {
				log.Fatalf("âŒ èŠ‚ç‚¹æ± ä¸­æ²¡æœ‰å¯ç”¨èŠ‚ç‚¹ï¼Œæ— æ³•ç»§ç»­å·¥ä½œ")
			}

			// ç”Ÿæˆä¸´æ—¶ clientIDï¼ˆä¸éœ€è¦æ³¨å†Œï¼‰
			clientID = fmt.Sprintf("client-%s-%d", cfg.Client.Name, time.Now().Unix())
			log.Printf("âœ… èŠ‚ç‚¹æ± å‡†å¤‡å®Œæˆï¼Œä½¿ç”¨ä¸´æ—¶å®¢æˆ·ç«¯ ID: %s", clientID)
		} else {
			// å¦‚æœæ²¡æœ‰é…ç½®èŠ‚ç‚¹åˆ—è¡¨ï¼Œä½¿ç”¨æœåŠ¡ç«¯å‘ç°æ¨¡å¼
			// åœ¨ both æ¨¡å¼ä¸‹ï¼Œç¡®ä¿ä½¿ç”¨ gRPC å®¢æˆ·ç«¯è¿›è¡Œæ³¨å†Œï¼ˆè€Œä¸æ˜¯ TUIC å®¢æˆ·ç«¯ï¼‰
			// å› ä¸ºåªæœ‰ gRPC æ”¯æŒè¿”å›èŠ‚ç‚¹åˆ—è¡¨
			var registerClient tasksmanager.TasksManagerClient
			if cfg.Protocol.Type == "both" && dualClient != nil {
				registerClient = dualClient.GetGRPCClient()
				log.Println("ğŸ”„ both æ¨¡å¼ï¼šä½¿ç”¨ gRPC å®¢æˆ·ç«¯è¿›è¡Œæ³¨å†Œï¼ˆç”¨äºè·å–èŠ‚ç‚¹åˆ—è¡¨ï¼‰")
			} else {
				registerClient = client
			}
			var regResp *tasksmanager.RegisterClientResponse
			var err error
			clientID, regResp, err = testNodeManagementWithResponse(ctx, registerClient, cfg.Client.Name)
			if err != nil {
				log.Printf("å®¢æˆ·ç«¯æ³¨å†Œå¤±è´¥: %v", err)
				return
			}

			// è¾“å‡ºæ³¨å†Œå“åº”ä¸­çš„èŠ‚ç‚¹ä¿¡æ¯
			if regResp != nil {
				if regResp.Success {
					log.Printf("âœ… å®¢æˆ·ç«¯æ³¨å†ŒæˆåŠŸï¼Œå“åº”ä¸­åŒ…å« %d ä¸ªæœåŠ¡å™¨èŠ‚ç‚¹", len(regResp.ServerNodes))
				} else {
					log.Printf("âš ï¸  å®¢æˆ·ç«¯æ³¨å†Œå“åº” Success=false")
				}
			} else {
				log.Printf("âš ï¸  å®¢æˆ·ç«¯æ³¨å†Œå“åº”ä¸ºç©º")
			}

			// å¤„ç†æ³¨å†Œå“åº”ï¼Œè‡ªåŠ¨è¿æ¥åˆ°æ‰€æœ‰æœåŠ¡å™¨èŠ‚ç‚¹
			if regResp != nil && regResp.Success && len(regResp.ServerNodes) > 0 {
				log.Printf("ğŸ“¡ å‘ç° %d ä¸ªæœåŠ¡å™¨èŠ‚ç‚¹ï¼Œå¼€å§‹è‡ªåŠ¨è¿æ¥åˆ°èŠ‚ç‚¹æ± ", len(regResp.ServerNodes))
				nodePool.AddNodes(regResp.ServerNodes)
				log.Printf("âœ… èŠ‚ç‚¹æ± åˆå§‹åŒ–å®Œæˆ: æ€»æ•°=%d, å¥åº·=%d", nodePool.GetNodeCount(), nodePool.GetHealthyNodeCount())
			} else {
				// å¦‚æœæ³¨å†Œå“åº”ä¸­æ²¡æœ‰èŠ‚ç‚¹åˆ—è¡¨ï¼Œå°è¯•é€šè¿‡ GetGrpcServerNodeInfoList è·å–
				log.Printf("âš ï¸  æ³¨å†Œå“åº”ä¸­æœªåŒ…å«èŠ‚ç‚¹åˆ—è¡¨ï¼Œå°è¯•é€šè¿‡ GetGrpcServerNodeInfoList è·å–...")
				nodeListResp, err := registerClient.GetGrpcServerNodeInfoList(ctx, &tasksmanager.GrpcServerNodeInfoListRequest{})
				if err != nil {
					log.Printf("âš ï¸  è·å–èŠ‚ç‚¹åˆ—è¡¨å¤±è´¥: %v", err)
				} else if len(nodeListResp.Items) > 0 {
					log.Printf("ğŸ“¡ é€šè¿‡ GetGrpcServerNodeInfoList å‘ç° %d ä¸ªæœåŠ¡å™¨èŠ‚ç‚¹", len(nodeListResp.Items))
					// è½¬æ¢ä¸º ServerNodes æ ¼å¼
					serverNodes := make([]*tasksmanager.GrpcServerNodeInfo, 0, len(nodeListResp.Items))
					for _, node := range nodeListResp.Items {
						serverNodes = append(serverNodes, node)
					}
					nodePool.AddNodes(serverNodes)
					log.Printf("âœ… èŠ‚ç‚¹æ± åˆå§‹åŒ–å®Œæˆ: æ€»æ•°=%d, å¥åº·=%d", nodePool.GetNodeCount(), nodePool.GetHealthyNodeCount())
				} else {
					log.Printf("âš ï¸  æœåŠ¡å™¨ä¸Šæš‚æ— å¯ç”¨èŠ‚ç‚¹")
				}
			}
		}

		// å¯åŠ¨èŠ‚ç‚¹æ± å¥åº·æ£€æŸ¥
		healthCtx, cancelHealth := context.WithCancel(context.Background())
		defer cancelHealth()
		go nodePool.StartHealthCheck(healthCtx, 30*time.Second)

		// å¯åŠ¨å®¢æˆ·ç«¯å¿ƒè·³ï¼ˆæŒç»­å‘ç°æ–°èŠ‚ç‚¹ï¼‰
		// å¦‚æœé…ç½®äº†èŠ‚ç‚¹åˆ—è¡¨ï¼Œä¸å¯åŠ¨å¿ƒè·³ï¼ˆå› ä¸ºä¸éœ€è¦å‘ç°æ–°èŠ‚ç‚¹ï¼‰
		if !hasNodeList {
			// åœ¨ both æ¨¡å¼ä¸‹ï¼Œç¡®ä¿ä½¿ç”¨ gRPC å®¢æˆ·ç«¯è¿›è¡Œå¿ƒè·³ï¼ˆè€Œä¸æ˜¯ TUIC å®¢æˆ·ç«¯ï¼‰
			// å› ä¸ºåªæœ‰ gRPC æ”¯æŒèŠ‚ç‚¹å‘ç°åŠŸèƒ½
			var heartbeatClient tasksmanager.TasksManagerClient
			if cfg.Protocol.Type == "both" && dualClient != nil {
				// both æ¨¡å¼ï¼šä½¿ç”¨ dualClient çš„ gRPC å®¢æˆ·ç«¯è¿›è¡Œå¿ƒè·³
				// ç›´æ¥ä½¿ç”¨ gRPC å®¢æˆ·ç«¯ï¼ˆç»•è¿‡ TUIC ä¼˜å…ˆé€»è¾‘ï¼Œç¡®ä¿èƒ½è·å–èŠ‚ç‚¹ä¿¡æ¯ï¼‰
				heartbeatClient = dualClient.GetGRPCClient()
				log.Println("ğŸ”„ both æ¨¡å¼ï¼šä½¿ç”¨ gRPC å®¢æˆ·ç«¯è¿›è¡Œå¿ƒè·³ï¼ˆç”¨äºèŠ‚ç‚¹å‘ç°ï¼‰")
			} else {
				heartbeatClient = client
			}
			go startHeartbeatWithNodePool(ctx, heartbeatClient, cfg.Client.Name, clientID, nodePool)
		} else {
			log.Println("ğŸ“¡ ä½¿ç”¨é…ç½®çš„èŠ‚ç‚¹åˆ—è¡¨ï¼Œè·³è¿‡å¿ƒè·³ï¼ˆä¸éœ€è¦å‘ç°æ–°èŠ‚ç‚¹ï¼‰")
		}

		// ç¡®ä¿èŠ‚ç‚¹æ± æœ‰å¯ç”¨èŠ‚ç‚¹åæ‰å¼€å§‹ä»»åŠ¡æäº¤
		if nodePool.GetHealthyNodeCount() == 0 {
			log.Fatalf("âŒ èŠ‚ç‚¹æ± ä¸­æ²¡æœ‰å¯ç”¨èŠ‚ç‚¹ï¼Œæ— æ³•æäº¤ä»»åŠ¡")
		}

		// æ ¹æ®åè®®ç±»å‹é€‰æ‹©ä»»åŠ¡æäº¤æ–¹å¼
		// æ— è®ºä»€ä¹ˆæ¨¡å¼ï¼Œéƒ½ä½¿ç”¨èŠ‚ç‚¹æ± è¿›è¡Œè´Ÿè½½å‡è¡¡
		log.Printf("ğŸš€ å¼€å§‹æäº¤ä»»åŠ¡ï¼ŒèŠ‚ç‚¹æ± çŠ¶æ€: æ€»æ•°=%d, å¥åº·=%d", nodePool.GetNodeCount(), nodePool.GetHealthyNodeCount())
		if cfg.Task.RepeatCount > 1 {
			if err := submitRealTaskMultipleTimesWithNodePool(ctx, nodePool, cfg.Protocol.Type, cfg.Client.Name, cfg.Task.TaskType, cfg.Task.TileKey, cfg.Task.Epoch, cfg.Task.RepeatCount, cfg.Task.Concurrency); err != nil {
				log.Fatalf("æ‰¹é‡æäº¤ä»»åŠ¡å¤±è´¥: %v", err)
			}
		} else {
			if err := submitRealTaskWithNodePool(ctx, nodePool, cfg.Protocol.Type, cfg.Client.Name, cfg.Task.TaskType, cfg.Task.TileKey, cfg.Task.Epoch); err != nil {
				log.Fatalf("æäº¤ä»»åŠ¡å¤±è´¥: %v", err)
			}
		}
	} else {
		// çº¯ TUIC æ¨¡å¼ï¼Œä½¿ç”¨åŸæœ‰é€»è¾‘
		if cfg.Task.RepeatCount > 1 {
			if err := submitRealTaskMultipleTimes(ctx, client, cfg.Client.Name, cfg.Task.TaskType, cfg.Task.TileKey, cfg.Task.Epoch, cfg.Task.RepeatCount, cfg.Task.Concurrency); err != nil {
				log.Fatalf("æ‰¹é‡æäº¤ä»»åŠ¡å¤±è´¥: %v", err)
			}
		} else {
			if err := submitRealTask(ctx, client, cfg.Client.Name, cfg.Task.TaskType, cfg.Task.TileKey, cfg.Task.Epoch); err != nil {
				log.Fatalf("æäº¤ä»»åŠ¡å¤±è´¥: %v", err)
			}
		}
	}

	// è¾“å‡ºåè®®æ¨¡å¼ä¿¡æ¯
	if cfg.Protocol.Type == "tuic" {
		log.Println("\n=== TUIC åè®®æ¨¡å¼ ===")
		log.Println("æç¤º: TUIC åè®®å½“å‰ä½¿ç”¨ HTTP æ¥å£æ¨¡å¼ï¼Œä¸æ”¯æŒèŠ‚ç‚¹ç®¡ç†å’Œå¿ƒè·³åŠŸèƒ½")
		log.Println("ä»»åŠ¡æäº¤åŠŸèƒ½å·²æµ‹è¯•å®Œæˆ")
	} else if cfg.Protocol.Type == "both" {
		log.Println("\n=== åŒåè®®æ¨¡å¼ï¼ˆèŠ‚ç‚¹æ± å·²åˆå§‹åŒ–ï¼‰===")
		log.Println("èŠ‚ç‚¹æ± åŠŸèƒ½: è‡ªåŠ¨å‘ç°å’Œè¿æ¥åˆ°å¤šä¸ªæœåŠ¡å™¨èŠ‚ç‚¹ï¼Œæ”¯æŒè´Ÿè½½å‡è¡¡")
		log.Println("ä»»åŠ¡æäº¤: ä½¿ç”¨èŠ‚ç‚¹æ± è¿›è¡Œè´Ÿè½½å‡è¡¡ï¼ˆé€šè¿‡ gRPC åè®®ï¼‰")
		log.Println("èŠ‚ç‚¹å‘ç°: æŒç»­é€šè¿‡å¿ƒè·³å‘ç°æ–°èŠ‚ç‚¹ï¼Œè‡ªåŠ¨æ·»åŠ åˆ°èŠ‚ç‚¹æ± ")
	} else {
		// gRPC åè®®ï¼šèŠ‚ç‚¹æ± å·²åœ¨ä¸Šé¢åˆå§‹åŒ–
		log.Println("\n=== gRPC åè®®æ¨¡å¼ï¼ˆèŠ‚ç‚¹æ± å·²åˆå§‹åŒ–ï¼‰===")
		log.Println("èŠ‚ç‚¹æ± åŠŸèƒ½: è‡ªåŠ¨å‘ç°å’Œè¿æ¥åˆ°å¤šä¸ªæœåŠ¡å™¨èŠ‚ç‚¹ï¼Œæ”¯æŒè´Ÿè½½å‡è¡¡")
	}

	// ç­‰å¾…ä¸­æ–­ä¿¡å·
	quit := make(chan os.Signal, 1)
	signal.Notify(quit, syscall.SIGINT, syscall.SIGTERM)
	<-quit

	log.Println("å®¢æˆ·ç«¯æ­£åœ¨å…³é—­...")
}

// testBasicConnection æµ‹è¯•åŸºç¡€è¿æ¥
func testBasicConnection(ctx context.Context, client tasksmanager.TasksManagerClient) error {
	// è·å–å®¢æˆ·ç«¯åˆ—è¡¨
	resp, err := client.GetTaskClientInfoList(ctx, &tasksmanager.TaskClientInfoListRequest{})
	if err != nil {
		return fmt.Errorf("è·å–å®¢æˆ·ç«¯åˆ—è¡¨å¤±è´¥: %w", err)
	}
	log.Printf("å½“å‰å®¢æˆ·ç«¯æ•°é‡: %d", len(resp.Items))

	// è·å–èŠ‚ç‚¹åˆ—è¡¨
	nodeResp, err := client.GetGrpcServerNodeInfoList(ctx, &tasksmanager.GrpcServerNodeInfoListRequest{})
	if err != nil {
		return fmt.Errorf("è·å–èŠ‚ç‚¹åˆ—è¡¨å¤±è´¥: %w", err)
	}
	log.Printf("å½“å‰èŠ‚ç‚¹æ•°é‡: %d", len(nodeResp.Items))

	return nil
}

// testNodeManagement æµ‹è¯•èŠ‚ç‚¹ç®¡ç†ï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰
func testNodeManagement(ctx context.Context, client tasksmanager.TasksManagerClient, clientName string) (string, error) {
	clientID, _, err := testNodeManagementWithResponse(ctx, client, clientName)
	return clientID, err
}

// testNodeManagementWithResponse å®¢æˆ·ç«¯æ³¨å†Œå¹¶è¿”å›å“åº”
func testNodeManagementWithResponse(ctx context.Context, client tasksmanager.TasksManagerClient, clientName string) (string, *tasksmanager.RegisterClientResponse, error) {
	// è·å–çœŸå®çš„ç³»ç»Ÿä¿¡æ¯
	_, systemInfo, cpuInfo, memoryInfo, _ := getRealSystemInfo()

	// åˆ›å»ºå®¢æˆ·ç«¯ä¿¡æ¯ï¼ˆä¸æ˜¯æœåŠ¡å™¨èŠ‚ç‚¹ä¿¡æ¯ï¼‰
	clientInfo := &tasksmanager.TaskClientInfo{
		ClientUuid:           fmt.Sprintf("client-%s-%d", clientName, time.Now().Unix()),
		ClientName:           clientName,
		ClientIp:             "127.0.0.1",
		ClientSystem:         systemInfo,
		ClientVersion:        "1.0.0",
		ClientCpu:            cpuInfo,
		ClientMemory:         memoryInfo,
		ClientCreateTime:     time.Now().Format(time.RFC3339),
		ClientLastActiveTime: time.Now().Format(time.RFC3339),
		ClientTaskStatus:     tasksmanager.ClientTaskStatus_CLIENT_TASK_STATUS_ONLINE,
	}

	// æ³¨å†Œå®¢æˆ·ç«¯ï¼ˆä½¿ç”¨å®¢æˆ·ç«¯ä¸“ç”¨æ¥å£ï¼‰
	regResp, err := client.RegisterClient(ctx, clientInfo)
	if err != nil {
		return "", nil, fmt.Errorf("å®¢æˆ·ç«¯æ³¨å†Œå¤±è´¥: %w", err)
	}

	if regResp.Success {
		log.Printf("å®¢æˆ·ç«¯æ³¨å†ŒæˆåŠŸ: %s", clientInfo.ClientUuid)
		if len(regResp.ServerNodes) > 0 {
			log.Printf("ğŸ“¡ å‘ç° %d ä¸ªæœåŠ¡å™¨èŠ‚ç‚¹ï¼Œå°†è‡ªåŠ¨è¿æ¥", len(regResp.ServerNodes))
		}
	}

	return clientInfo.ClientUuid, regResp, nil
}

// startHeartbeatWithNodePool å¯åŠ¨å®¢æˆ·ç«¯å¿ƒè·³ï¼ˆåŒ…å«èŠ‚ç‚¹æ± ç®¡ç†å™¨ï¼‰
func startHeartbeatWithNodePool(ctx context.Context, client tasksmanager.TasksManagerClient, clientName, clientID string, nodePool *NodePool) {
	ticker := time.NewTicker(10 * time.Second)
	defer ticker.Stop()

	// åˆå§‹åŒ–ç³»ç»Ÿä¿¡æ¯
	_, systemInfo, cpuInfo, memoryInfo, _ := getRealSystemInfo()
	firstRun := true

	for range ticker.C {
		// è·å–çœŸå®çš„ç³»ç»Ÿä¿¡æ¯
		cpuUsage, err := getCPUUsage()
		if err != nil {
			log.Printf("è·å– CPU ä½¿ç”¨ç‡å¤±è´¥: %v", err)
			cpuUsage = 0
		}

		memoryUsed, memoryTotal, err := getMemoryUsage()
		if err != nil {
			log.Printf("è·å–å†…å­˜ä½¿ç”¨æƒ…å†µå¤±è´¥: %v", err)
			memoryUsed = 0
			memoryTotal = 0
		}

		// è·å–ç½‘ç»œä½¿ç”¨æƒ…å†µï¼ˆé¦–æ¬¡è°ƒç”¨éœ€è¦ç­‰å¾…1ç§’ï¼‰
		networkRx, networkTx, err := getNetworkUsage()
		if err != nil {
			if firstRun {
				// é¦–æ¬¡è¿è¡Œéœ€è¦åˆå§‹åŒ–ï¼Œè·³è¿‡æœ¬æ¬¡
				firstRun = false
				continue
			}
			log.Printf("è·å–ç½‘ç»œä½¿ç”¨æƒ…å†µå¤±è´¥: %v", err)
			networkRx = 0
			networkTx = 0
		}
		firstRun = false

		// è·å–ç£ç›˜ä½¿ç”¨æƒ…å†µ
		diskUsed, diskTotal, err := getDiskUsage()
		if err != nil {
			log.Printf("è·å–ç£ç›˜ä½¿ç”¨æƒ…å†µå¤±è´¥: %v", err)
			diskUsed = 0
			diskTotal = 0
		}

		// åˆ›å»ºå®¢æˆ·ç«¯ä¿¡æ¯ï¼ˆä¸æ˜¯æœåŠ¡å™¨èŠ‚ç‚¹ä¿¡æ¯ï¼‰
		clientInfo := &tasksmanager.TaskClientInfo{
			ClientUuid:           clientID,
			ClientName:           clientName,
			ClientIp:             "127.0.0.1",
			ClientSystem:         systemInfo,
			ClientVersion:        "1.0.0",
			ClientCpu:            cpuInfo,
			ClientMemory:         memoryInfo,
			ClientCreateTime:     time.Now().Format(time.RFC3339),
			ClientLastActiveTime: time.Now().Format(time.RFC3339),
			ClientTaskStatus:     tasksmanager.ClientTaskStatus_CLIENT_TASK_STATUS_ONLINE,
			CpuUsagePercent:      &cpuUsage,
			MemoryUsedBytes:      &memoryUsed,
			MemoryTotalBytes:     &memoryTotal,
			NetworkRxBytesPerSec: &networkRx,
			NetworkTxBytesPerSec: &networkTx,
			DiskUsedBytes:        &diskUsed,
			DiskTotalBytes:       &diskTotal,
		}

		updateTime := time.Now().Format(time.RFC3339)
		clientInfo.ResourceUpdateTime = &updateTime

		// ä½¿ç”¨å®¢æˆ·ç«¯å¿ƒè·³æ¥å£ï¼ˆä¸æ˜¯æœåŠ¡å™¨èŠ‚ç‚¹å¿ƒè·³ï¼‰
		resp, err := client.ClientHeartbeat(ctx, clientInfo)
		if err != nil {
			log.Printf("å®¢æˆ·ç«¯å¿ƒè·³å‘é€å¤±è´¥: %v", err)
			continue
		}

		if resp.Success {
			memPercent := float64(0)
			if memoryTotal > 0 {
				memPercent = float64(memoryUsed) / float64(memoryTotal) * 100
			}
			log.Printf("å®¢æˆ·ç«¯å¿ƒè·³å‘é€æˆåŠŸ: %s (CPU: %.1f%%, Memory: %.1f%% [%.2fGB/%.2fGB], ç½‘ç»œ: â†“%.2fMB/s â†‘%.2fMB/s)",
				clientID, cpuUsage, memPercent,
				float64(memoryUsed)/1024/1024/1024, float64(memoryTotal)/1024/1024/1024,
				networkRx/1024/1024, networkTx/1024/1024)

			// å¤„ç†å¿ƒè·³å“åº”ä¸­çš„æ–°æœåŠ¡å™¨èŠ‚ç‚¹ä¿¡æ¯
			if nodePool != nil {
				if len(resp.NewServerNodes) > 0 {
					oldCount := nodePool.GetNodeCount()
					log.Printf("ğŸ“¡ å¿ƒè·³å“åº”ä¸­å‘ç° %d ä¸ªæ–°æœåŠ¡å™¨èŠ‚ç‚¹ï¼Œæ­£åœ¨æ·»åŠ åˆ°èŠ‚ç‚¹æ± ...", len(resp.NewServerNodes))
					for _, node := range resp.NewServerNodes {
						log.Printf("  - æ–°èŠ‚ç‚¹: %s (%s:%s)", node.NodeUuid, node.NodeIp, node.NodePort)
					}
					nodePool.AddNodes(resp.NewServerNodes)
					newCount := nodePool.GetNodeCount()
					log.Printf("âœ… èŠ‚ç‚¹æ± æ›´æ–°å®Œæˆ: æ€»æ•°=%d (æ–°å¢ %d ä¸ª), å¥åº·=%d", newCount, newCount-oldCount, nodePool.GetHealthyNodeCount())
					log.Printf("ğŸ”„ æ–°èŠ‚ç‚¹å·²åŠ å…¥è´Ÿè½½å‡è¡¡ï¼Œåç»­ä»»åŠ¡å°†è‡ªåŠ¨åˆ†æ‹…åˆ°æ‰€æœ‰èŠ‚ç‚¹")
				} else {
					// å³ä½¿æ²¡æœ‰æ–°èŠ‚ç‚¹ï¼Œä¹Ÿå®šæœŸè¾“å‡ºèŠ‚ç‚¹æ± çŠ¶æ€ï¼ˆæ¯10æ¬¡å¿ƒè·³è¾“å‡ºä¸€æ¬¡ï¼‰
					// è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œæ¯æ¬¡å¿ƒè·³éƒ½è¾“å‡ºå½“å‰èŠ‚ç‚¹æ± çŠ¶æ€ï¼ˆDEBUGçº§åˆ«ï¼‰
					log.Printf("[DEBUG] èŠ‚ç‚¹æ± å½“å‰çŠ¶æ€: æ€»æ•°=%d, å¥åº·=%d", nodePool.GetNodeCount(), nodePool.GetHealthyNodeCount())
				}
			}
		}
	}
}

// startHeartbeat å¯åŠ¨å¿ƒè·³ï¼ˆæ—§ç‰ˆæœ¬ï¼Œä¿æŒå‘åå…¼å®¹ï¼‰
func startHeartbeat(ctx context.Context, client tasksmanager.TasksManagerClient, clientName string) {
	clientID := fmt.Sprintf("client-%s-%d", clientName, time.Now().Unix())
	startHeartbeatWithNodePool(ctx, client, clientName, clientID, nil)
}

// submitRealTask æäº¤çœŸå®çš„æ•°æ®è¯·æ±‚ä»»åŠ¡
func submitRealTask(ctx context.Context, client tasksmanager.TasksManagerClient, clientID, taskTypeStr, tileKey string, epoch int32) error {
	// è§£æä»»åŠ¡ç±»å‹
	var taskType tasksmanager.TaskType
	switch taskTypeStr {
	case "q2":
		taskType = tasksmanager.TaskType_TASK_TYPE_GOOGLE_EARTH_Q2
	case "imagery":
		taskType = tasksmanager.TaskType_TASK_TYPE_GOOGLE_EARTH_IMAGERY
	case "terrain":
		taskType = tasksmanager.TaskType_TASK_TYPE_GOOGLE_EARTH_TERRAIN
	default:
		return fmt.Errorf("ä¸æ”¯æŒçš„ä»»åŠ¡ç±»å‹: %s (æ”¯æŒ: q2, imagery, terrain)", taskTypeStr)
	}

	taskMethod := tasksmanager.TaskMethod_TASK_METHOD_GET
	taskStatus := tasksmanager.TaskStatus_TASK_STATUS_PENDING

	// ä½¿ç”¨åå°„åˆ›å»ºè¯·æ±‚ï¼ˆå› ä¸º proto å¯èƒ½è¿˜æœªé‡æ–°ç”Ÿæˆï¼‰
	req := &tasksmanager.TaskRequest{
		TaskClientId: clientID,
		TaskType:     taskType,
		TaskMethod:   &taskMethod,
		TaskStatus:   &taskStatus,
	}

	// è®¾ç½® TileKey å’Œ Epoch å­—æ®µï¼ˆproto æ–‡ä»¶å·²é‡æ–°ç”Ÿæˆï¼‰
	req.TileKey = tileKey
	req.Epoch = epoch

	log.Printf("æäº¤ä»»åŠ¡è¯·æ±‚: task_type=%s, TileKey=%s, epoch=%d", taskTypeStr, tileKey, epoch)

	// å‘é€ä»»åŠ¡è¯·æ±‚ï¼ˆå•æ¬¡è¯·æ±‚ï¼ŒçŠ¶æ€ç é 200 è§†ä¸ºå¤±è´¥ï¼‰
	startTime := time.Now()
	resp, err := client.SubmitTask(ctx, req)
	elapsed := time.Since(startTime)

	if err != nil {
		return fmt.Errorf("æäº¤ä»»åŠ¡å¤±è´¥: %w", err)
	}

	// æ‰“å°ç»“æœ
	log.Println()
	log.Println("=== ä»»åŠ¡æ‰§è¡Œç»“æœ ===")
	statusCode := getResponseStatusCode(resp)
	log.Printf("çŠ¶æ€ç : %d", statusCode)

	bodySize := getResponseBodySize(resp)
	log.Printf("å“åº”ä½“å¤§å°: %d å­—èŠ‚ (%.2f KB, %.2f MB)",
		bodySize,
		float64(bodySize)/1024,
		float64(bodySize)/1024/1024)
	log.Printf("è¯·æ±‚è€—æ—¶: %v", elapsed)

	// è·å–å“åº”ä¸­çš„ TileKey å’Œ Epoch
	log.Printf("å“åº” TileKey: %s", resp.TileKey)
	log.Printf("å“åº” Epoch: %d", resp.Epoch)

	if statusCode != 200 {
		return fmt.Errorf("ä»»åŠ¡è¿”å›é 200 çŠ¶æ€ç : %d", statusCode)
	}

	return nil
}

// getResponseStatusCode è·å–å“åº”çŠ¶æ€ç 
func getResponseStatusCode(resp *tasksmanager.TaskResponse) int32 {
	if resp.TaskResponseStatusCode != nil {
		return *resp.TaskResponseStatusCode
	}
	return 0
}

// getResponseBodySize è·å–å“åº”ä½“å¤§å°
func getResponseBodySize(resp *tasksmanager.TaskResponse) int {
	if resp.TaskResponseBody != nil {
		return len(resp.TaskResponseBody)
	}
	return 0
}

// ï¼ˆå®¢æˆ·ç«¯ä¾§é‡è¯•é€»è¾‘å·²ç§»é™¤ï¼Œæ˜¯å¦é‡è¯•ç”±è°ƒç”¨æ–¹æˆ–æœåŠ¡ç«¯æ§åˆ¶ï¼‰

// submitRealTaskMultipleTimes é‡å¤æäº¤åŒä¸€ä¸ªä»»åŠ¡è¯·æ±‚å¤šæ¬¡ï¼ˆç”¨äºæ€§èƒ½æµ‹è¯•ï¼‰
func submitRealTaskMultipleTimes(ctx context.Context, client tasksmanager.TasksManagerClient, clientID, taskTypeStr, tileKey string, epoch int32, repeatCount, concurrency int) error {
	// è§£æä»»åŠ¡ç±»å‹
	var taskType tasksmanager.TaskType
	switch taskTypeStr {
	case "q2":
		taskType = tasksmanager.TaskType_TASK_TYPE_GOOGLE_EARTH_Q2
	case "imagery":
		taskType = tasksmanager.TaskType_TASK_TYPE_GOOGLE_EARTH_IMAGERY
	case "terrain":
		taskType = tasksmanager.TaskType_TASK_TYPE_GOOGLE_EARTH_TERRAIN
	default:
		return fmt.Errorf("ä¸æ”¯æŒçš„ä»»åŠ¡ç±»å‹: %s (æ”¯æŒ: q2, imagery, terrain)", taskTypeStr)
	}

	taskMethod := tasksmanager.TaskMethod_TASK_METHOD_GET
	taskStatus := tasksmanager.TaskStatus_TASK_STATUS_PENDING

	// åˆ›å»ºè¯·æ±‚
	req := &tasksmanager.TaskRequest{
		TaskClientId: clientID,
		TaskType:     taskType,
		TaskMethod:   &taskMethod,
		TaskStatus:   &taskStatus,
		TileKey:      tileKey,
		Epoch:        epoch,
	}

	log.Printf("å¼€å§‹æ‰¹é‡æäº¤ä»»åŠ¡: task_type=%s, TileKey=%s, epoch=%d, é‡å¤æ¬¡æ•°=%d", taskTypeStr, tileKey, epoch, repeatCount)
	log.Println()

	// ç»Ÿè®¡å˜é‡
	var (
		completedTasks   int64
		failedTasks      int64
		totalBytes       int64
		firstRequestTime time.Duration
		firstRequestOnce sync.Once // ç¡®ä¿åªè®°å½•ç¬¬ä¸€æ¬¡è¯·æ±‚æ—¶é—´
		requestTimes     []time.Duration
		requestTimesMu   sync.Mutex
	)

	// è®°å½•æ€»å¼€å§‹æ—¶é—´
	totalStartTime := time.Now()

	// é«˜å¹¶å‘å‘é€è¯·æ±‚
	// ä¼˜åŒ–ï¼šæ ¹æ®ä»»åŠ¡æ•°é‡åŠ¨æ€è°ƒæ•´é»˜è®¤å¹¶å‘æ•°
	if concurrency <= 0 {
		// é»˜è®¤å¹¶å‘æ•°ï¼šæ ¹æ®ä»»åŠ¡æ•°é‡åŠ¨æ€è°ƒæ•´
		if repeatCount < 1000 {
			concurrency = 100
		} else if repeatCount < 10000 {
			concurrency = 500
		} else {
			concurrency = 1000
		}
	}
	if repeatCount < concurrency {
		concurrency = repeatCount
	}

	log.Printf("å¹¶å‘é…ç½®: %d ä¸ªå·¥ä½œ goroutine, æ€»ä»»åŠ¡æ•°: %d", concurrency, repeatCount)

	// åˆ›å»ºä»»åŠ¡é€šé“å’Œå·¥ä½œ goroutine
	taskChan := make(chan int, repeatCount)
	var wg sync.WaitGroup

	// å¯åŠ¨å·¥ä½œ goroutine
	for i := 0; i < concurrency; i++ {
		wg.Add(1)
		go func(workerID int) {
			defer wg.Done()
			for taskID := range taskChan {
				startTime := time.Now()
				resp, err := client.SubmitTask(ctx, req)
				elapsed := time.Since(startTime)

				if err != nil {
					atomic.AddInt64(&failedTasks, 1)
					log.Printf("âŒ [Worker %d] è¯·æ±‚ #%d å¤±è´¥: %v", workerID, taskID+1, err)
					continue
				}

				statusCode := getResponseStatusCode(resp)
				if statusCode != 200 {
					atomic.AddInt64(&failedTasks, 1)
					log.Printf("âŒ [Worker %d] è¯·æ±‚ #%d è¿”å›é 200 çŠ¶æ€ç : %d", workerID, taskID+1, statusCode)
					continue
				}

				atomic.AddInt64(&completedTasks, 1)

				// è®°å½•ç¬¬ä¸€æ¬¡è¯·æ±‚çš„æ—¶é—´ï¼ˆä½¿ç”¨ sync.Once ç¡®ä¿çº¿ç¨‹å®‰å…¨ï¼‰
				firstRequestOnce.Do(func() {
					firstRequestTime = elapsed
				})

				// ä¿å­˜è¯·æ±‚è€—æ—¶
				requestTimesMu.Lock()
				requestTimes = append(requestTimes, elapsed)
				requestTimesMu.Unlock()

				// ç»Ÿè®¡å“åº”ä½“å¤§å°
				if resp.TaskResponseBody != nil {
					atomic.AddInt64(&totalBytes, int64(len(resp.TaskResponseBody)))
				}

				// æ¯æ¬¡æˆåŠŸè¯·æ±‚è¾“å‡ºè¿›åº¦
				log.Printf("âœ… [Worker %d] è¯·æ±‚ #%d: çŠ¶æ€ç =%d, è€—æ—¶=%v, å“åº”å¤§å°=%d å­—èŠ‚",
					workerID, taskID+1, statusCode, elapsed, getResponseBodySize(resp))

				// å¦‚æœæ˜¯ç¬¬ä¸€æ¬¡è¯·æ±‚ï¼Œè¾“å‡ºè¯¦ç»†ä¿¡æ¯
				if taskID == 0 {
					log.Printf("   é¦–æ¬¡è¯·æ±‚: TileKey=%s, Epoch=%d", resp.TileKey, resp.Epoch)
				}
			}
		}(i)
	}

	// å‘é€æ‰€æœ‰ä»»åŠ¡
	for i := 0; i < repeatCount; i++ {
		taskChan <- i
	}
	close(taskChan)

	// ç­‰å¾…æ‰€æœ‰ goroutine å®Œæˆ
	wg.Wait()

	totalElapsed := time.Since(totalStartTime)
	completed := atomic.LoadInt64(&completedTasks)
	failed := atomic.LoadInt64(&failedTasks)
	totalBytesCount := atomic.LoadInt64(&totalBytes)

	// è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
	var (
		avgTime    time.Duration
		minTime    time.Duration
		maxTime    time.Duration
		medianTime time.Duration
	)

	if len(requestTimes) > 0 {
		// è®¡ç®—å¹³å‡ã€æœ€å°ã€æœ€å¤§è€—æ—¶
		var sum time.Duration
		minTime = requestTimes[0]
		maxTime = requestTimes[0]

		for _, t := range requestTimes {
			sum += t
			if t < minTime {
				minTime = t
			}
			if t > maxTime {
				maxTime = t
			}
		}
		avgTime = sum / time.Duration(len(requestTimes))

		// è®¡ç®—ä¸­ä½æ•°
		sortedTimes := make([]time.Duration, len(requestTimes))
		copy(sortedTimes, requestTimes)
		// ç®€å•æ’åºï¼ˆå†’æ³¡æ’åºï¼Œæ•°é‡ä¸å¤šæ—¶è¶³å¤Ÿç”¨ï¼‰
		for i := 0; i < len(sortedTimes)-1; i++ {
			for j := i + 1; j < len(sortedTimes); j++ {
				if sortedTimes[i] > sortedTimes[j] {
					sortedTimes[i], sortedTimes[j] = sortedTimes[j], sortedTimes[i]
				}
			}
		}
		if len(sortedTimes) > 0 {
			medianTime = sortedTimes[len(sortedTimes)/2]
		}
	}

	// è¾“å‡ºç»Ÿè®¡ç»“æœ
	log.Println()
	log.Println("=" + strings.Repeat("=", 60))
	log.Println("=== æ‰¹é‡è¯·æ±‚æ€§èƒ½ç»Ÿè®¡ ===")
	log.Printf("æ€»è¯·æ±‚æ•°: %d", repeatCount)
	log.Printf("æˆåŠŸ: %d", completed)
	log.Printf("å¤±è´¥: %d", failed)
	log.Printf("æ€»è€—æ—¶: %v", totalElapsed)
	log.Printf("å¹³å‡ QPS: %.2f è¯·æ±‚/ç§’", float64(completed)/totalElapsed.Seconds())
	log.Printf("æ€»ä¼ è¾“æ•°æ®: %.2f KB (%.2f MB)", float64(totalBytesCount)/1024, float64(totalBytesCount)/1024/1024)
	log.Println()
	log.Println("--- è¯·æ±‚è€—æ—¶ç»Ÿè®¡ ---")
	if firstRequestTime > 0 {
		log.Printf("é¦–æ¬¡è¯·æ±‚è€—æ—¶: %v", firstRequestTime)
	}
	if avgTime > 0 {
		log.Printf("å¹³å‡è€—æ—¶: %v", avgTime)
	}
	if minTime > 0 {
		log.Printf("æœ€å¿«è¯·æ±‚: %v", minTime)
	}
	if maxTime > 0 {
		log.Printf("æœ€æ…¢è¯·æ±‚: %v", maxTime)
	}
	if medianTime > 0 {
		log.Printf("ä¸­ä½æ•°è€—æ—¶: %v", medianTime)
	}

	// åˆ†æè¿æ¥å¤ç”¨æ•ˆæœ
	if len(requestTimes) >= 2 && firstRequestTime > 0 {
		// è®¡ç®—ç¬¬äºŒæ¬¡åŠåç»­è¯·æ±‚çš„å¹³å‡è€—æ—¶
		subsequentTimes := requestTimes[1:]
		if len(subsequentTimes) > 0 {
			var subsequentSum time.Duration
			for _, t := range subsequentTimes {
				subsequentSum += t
			}
			avgSubsequentTime := subsequentSum / time.Duration(len(subsequentTimes))

			log.Println()
			log.Println("--- è¿æ¥å¤ç”¨æ•ˆæœåˆ†æ ---")
			log.Printf("é¦–æ¬¡è¯·æ±‚è€—æ—¶: %v", firstRequestTime)
			log.Printf("åç»­è¯·æ±‚å¹³å‡è€—æ—¶: %v (å…± %d ä¸ª)", avgSubsequentTime, len(subsequentTimes))
			if avgSubsequentTime < firstRequestTime {
				improvement := float64(firstRequestTime-avgSubsequentTime) / float64(firstRequestTime) * 100
				log.Printf("âœ… åç»­è¯·æ±‚åŠ é€Ÿ: %.1f%% (è¿æ¥å¤ç”¨ç”Ÿæ•ˆ)", improvement)
			} else {
				log.Printf("âš ï¸  åç»­è¯·æ±‚æœªåŠ é€Ÿï¼Œå¯èƒ½éœ€è¦æ£€æŸ¥è¿æ¥æ± é…ç½®")
			}
		}
	}

	log.Println("=" + strings.Repeat("=", 60))

	return nil
}

// submitRealTaskWithNodePool ä½¿ç”¨èŠ‚ç‚¹æ± æäº¤ä»»åŠ¡ï¼ˆè´Ÿè½½å‡è¡¡ï¼‰
func submitRealTaskWithNodePool(ctx context.Context, nodePool *NodePool, protocolType, clientID, taskTypeStr, tileKey string, epoch int32) error {
	// é€‰æ‹©èŠ‚ç‚¹
	node, err := nodePool.SelectNode()
	if err != nil {
		return fmt.Errorf("é€‰æ‹©èŠ‚ç‚¹å¤±è´¥: %w", err)
	}

	// å¦‚æœæ˜¯ both æ¨¡å¼ï¼Œä¼˜å…ˆä½¿ç”¨ TUIC åè®®
	if protocolType == "both" {
		// å°è¯•è·å– TUIC å®¢æˆ·ç«¯
		tuicClient, err := nodePool.GetTUICClient(node.GrpcInfo.NodeUuid)
		if err == nil && tuicClient != nil {
			// ä½¿ç”¨ TUIC å®¢æˆ·ç«¯æäº¤ä»»åŠ¡
			log.Printf("ä½¿ç”¨èŠ‚ç‚¹ %s (%s:%s) æäº¤ä»»åŠ¡ï¼ˆTUIC åè®®ï¼‰", node.GrpcInfo.NodeUuid, node.GrpcInfo.NodeIp, node.GrpcInfo.NodePort)
			tuicAdapter := newTUICClientAdapter(tuicClient)
			return submitRealTask(ctx, tuicAdapter, clientID, taskTypeStr, tileKey, epoch)
		}
		// TUIC ä¸å¯ç”¨ï¼Œå›é€€åˆ° gRPC
		log.Printf("èŠ‚ç‚¹ %s TUIC ä¸å¯ç”¨ï¼Œä½¿ç”¨ gRPC åè®®", node.GrpcInfo.NodeUuid)
	}

	// è·å– gRPC å®¢æˆ·ç«¯
	grpcClient, err := nodePool.GetGRPCClient(node.GrpcInfo.NodeUuid)
	if err != nil {
		return fmt.Errorf("è·å– gRPC å®¢æˆ·ç«¯å¤±è´¥: %w", err)
	}

	// ä½¿ç”¨é€‰ä¸­çš„èŠ‚ç‚¹æäº¤ä»»åŠ¡
	log.Printf("ä½¿ç”¨èŠ‚ç‚¹ %s (%s:%s) æäº¤ä»»åŠ¡ï¼ˆgRPC åè®®ï¼‰", node.GrpcInfo.NodeUuid, node.GrpcInfo.NodeIp, node.GrpcInfo.NodePort)
	return submitRealTask(ctx, grpcClient, clientID, taskTypeStr, tileKey, epoch)
}

// submitRealTaskMultipleTimesWithNodePool ä½¿ç”¨èŠ‚ç‚¹æ± æ‰¹é‡æäº¤ä»»åŠ¡ï¼ˆè´Ÿè½½å‡è¡¡ï¼‰
func submitRealTaskMultipleTimesWithNodePool(ctx context.Context, nodePool *NodePool, protocolType, clientID, taskTypeStr, tileKey string, epoch int32, repeatCount, concurrency int) error {
	// è§£æä»»åŠ¡ç±»å‹
	var taskType tasksmanager.TaskType
	switch taskTypeStr {
	case "q2":
		taskType = tasksmanager.TaskType_TASK_TYPE_GOOGLE_EARTH_Q2
	case "imagery":
		taskType = tasksmanager.TaskType_TASK_TYPE_GOOGLE_EARTH_IMAGERY
	case "terrain":
		taskType = tasksmanager.TaskType_TASK_TYPE_GOOGLE_EARTH_TERRAIN
	default:
		return fmt.Errorf("ä¸æ”¯æŒçš„ä»»åŠ¡ç±»å‹: %s (æ”¯æŒ: q2, imagery, terrain)", taskTypeStr)
	}

	taskMethod := tasksmanager.TaskMethod_TASK_METHOD_GET
	taskStatus := tasksmanager.TaskStatus_TASK_STATUS_PENDING

	// åˆ›å»ºè¯·æ±‚
	req := &tasksmanager.TaskRequest{
		TaskClientId: clientID,
		TaskType:     taskType,
		TaskMethod:   &taskMethod,
		TaskStatus:   &taskStatus,
		TileKey:      tileKey,
		Epoch:        epoch,
	}

	log.Printf("å¼€å§‹æ‰¹é‡æäº¤ä»»åŠ¡ï¼ˆä½¿ç”¨èŠ‚ç‚¹æ± è´Ÿè½½å‡è¡¡ï¼‰: task_type=%s, TileKey=%s, epoch=%d, é‡å¤æ¬¡æ•°=%d", taskTypeStr, tileKey, epoch, repeatCount)
	log.Printf("èŠ‚ç‚¹æ± çŠ¶æ€: æ€»æ•°=%d, å¥åº·=%d", nodePool.GetNodeCount(), nodePool.GetHealthyNodeCount())
	log.Println()

	// ç»Ÿè®¡å˜é‡
	var (
		completedTasks   int64
		failedTasks      int64
		totalBytes       int64
		firstRequestTime time.Duration
		firstRequestOnce sync.Once
		// ä¼˜åŒ–ï¼šä½¿ç”¨é¢„åˆ†é…çš„sliceï¼Œé¿å…é¢‘ç¹appendå’Œé”ç«äº‰
		// åªé‡‡æ ·éƒ¨åˆ†è¯·æ±‚çš„è€—æ—¶ï¼ˆæ¯10ä¸ªé‡‡æ ·1ä¸ªï¼‰ï¼Œå‡å°‘å†…å­˜å’Œé”å¼€é”€
		requestTimes   = make([]time.Duration, 0, repeatCount/10+100)
		requestTimesMu sync.Mutex
		// ä¼˜åŒ–ï¼šé¢„å…ˆåˆå§‹åŒ–æ‰€æœ‰èŠ‚ç‚¹çš„ä½¿ç”¨è®¡æ•°å™¨ï¼Œé¿å…è¿è¡Œæ—¶åŠ é”
		nodeUsage   = make(map[string]*int64) // èŠ‚ç‚¹ä½¿ç”¨ç»Ÿè®¡ï¼ˆä½¿ç”¨åŸå­æ“ä½œï¼‰
		nodeUsageMu sync.Mutex
		// å…¨å±€èŠ‚ç‚¹é€‰æ‹©è®¡æ•°å™¨ï¼ˆæ‰€æœ‰workerå…±äº«ï¼Œç¡®ä¿çœŸæ­£çš„è½®è¯¢ï¼‰
		globalNodeIndex int64
	)

	// è®°å½•æ€»å¼€å§‹æ—¶é—´
	totalStartTime := time.Now()

	// é«˜å¹¶å‘å‘é€è¯·æ±‚
	// ä¼˜åŒ–ï¼šæ ¹æ®ä»»åŠ¡æ•°é‡å’ŒèŠ‚ç‚¹æ•°é‡åŠ¨æ€è°ƒæ•´é»˜è®¤å¹¶å‘æ•°ï¼Œå……åˆ†åˆ©ç”¨å¤šèŠ‚ç‚¹èµ„æº
	if concurrency <= 0 {
		// è·å–å¯ç”¨èŠ‚ç‚¹æ•°é‡
		healthyNodeCount := nodePool.GetHealthyNodeCount()
		if healthyNodeCount == 0 {
			healthyNodeCount = 1 // é˜²æ­¢é™¤é›¶
		}

		// é»˜è®¤å¹¶å‘æ•°ï¼šæ ¹æ®ä»»åŠ¡æ•°é‡å’ŒèŠ‚ç‚¹æ•°é‡åŠ¨æ€è°ƒæ•´
		// åŸºç¡€å¹¶å‘æ•°ï¼šæ ¹æ®ä»»åŠ¡æ•°é‡
		var baseConcurrency int
		if repeatCount < 1000 {
			// å°ä»»åŠ¡ï¼šæ¯ä¸ªèŠ‚ç‚¹è‡³å°‘100å¹¶å‘ï¼Œå……åˆ†åˆ©ç”¨å¤šèŠ‚ç‚¹
			baseConcurrency = 200 // æé«˜å°ä»»åŠ¡çš„å¹¶å‘æ•°
		} else if repeatCount < 10000 {
			baseConcurrency = 500
		} else {
			baseConcurrency = 1000
		}

		// æ ¹æ®èŠ‚ç‚¹æ•°é‡è°ƒæ•´ï¼šå¤šèŠ‚ç‚¹æ—¶å¯ä»¥æ”¯æŒæ›´é«˜çš„å¹¶å‘
		// æ¯ä¸ªèŠ‚ç‚¹å¯ä»¥å¤„ç†æ›´å¤šå¹¶å‘è¯·æ±‚ï¼ˆTUIC/QUICæ”¯æŒå¤šè·¯å¤ç”¨ï¼‰
		// èŠ‚ç‚¹æ•°è¶Šå¤šï¼Œæ€»å¹¶å‘æ•°å¯ä»¥æ›´é«˜
		concurrency = baseConcurrency * healthyNodeCount

		// è®¾ç½®ä¸Šé™ï¼šé¿å…åˆ›å»ºè¿‡å¤šgoroutine
		// å¯¹äºå°ä»»åŠ¡ï¼Œå…è®¸æ›´é«˜çš„å¹¶å‘ä¸Šé™ï¼ˆå……åˆ†åˆ©ç”¨å¤šèŠ‚ç‚¹ï¼‰
		var maxConcurrency int
		if repeatCount < 1000 {
			maxConcurrency = 500 * healthyNodeCount // å°ä»»åŠ¡å…è®¸æ›´é«˜å¹¶å‘
		} else {
			maxConcurrency = 2000 * healthyNodeCount
		}
		if concurrency > maxConcurrency {
			concurrency = maxConcurrency
		}
	}
	if repeatCount < concurrency {
		concurrency = repeatCount
	}

	log.Printf("å¹¶å‘é…ç½®: %d ä¸ªå·¥ä½œ goroutine, æ€»ä»»åŠ¡æ•°: %d", concurrency, repeatCount)

	// é¢„å…ˆè·å–æ‰€æœ‰èŠ‚ç‚¹çš„å®¢æˆ·ç«¯å¼•ç”¨ï¼Œé¿å…æ¯æ¬¡è¯·æ±‚éƒ½åŠ é”
	type nodeClient struct {
		nodeUUID   string
		tuicClient TUICClient
		grpcClient tasksmanager.TasksManagerClient
		hasTUIC    bool
		nodeAddr   string
		tuicAddr   string
	}
	nodeClients := make([]*nodeClient, 0)
	nodePool.nodesMu.RLock()
	for uuid, node := range nodePool.nodes {
		if !node.Healthy {
			continue
		}
		nc := &nodeClient{
			nodeUUID: uuid,
			nodeAddr: fmt.Sprintf("%s:%s", node.GrpcInfo.NodeIp, node.GrpcInfo.NodePort),
		}

		// è·å– gRPC å®¢æˆ·ç«¯
		nodePool.grpcClientsMu.RLock()
		if grpcClient, exists := nodePool.grpcClients[uuid]; exists {
			nc.grpcClient = grpcClient
		}
		nodePool.grpcClientsMu.RUnlock()

		// è·å– TUIC å®¢æˆ·ç«¯
		if protocolType == "both" {
			nodePool.tuicClientsMu.RLock()
			if tuicClient, exists := nodePool.tuicClients[uuid]; exists {
				nc.tuicClient = tuicClient
				nc.hasTUIC = true
				if node.TUICConfig != nil {
					nc.tuicAddr = fmt.Sprintf("%s:%s", node.GrpcInfo.NodeIp, node.TUICConfig.Port)
				}
			}
			nodePool.tuicClientsMu.RUnlock()
		}

		if nc.grpcClient != nil {
			nodeClients = append(nodeClients, nc)
		}
	}
	nodePool.nodesMu.RUnlock()

	if len(nodeClients) == 0 {
		return fmt.Errorf("æ²¡æœ‰å¯ç”¨çš„èŠ‚ç‚¹å®¢æˆ·ç«¯")
	}

	// é¢„å…ˆåˆå§‹åŒ–æ‰€æœ‰èŠ‚ç‚¹çš„ä½¿ç”¨è®¡æ•°å™¨ï¼Œé¿å…è¿è¡Œæ—¶åŠ é”
	nodeUsageMu.Lock()
	for _, nc := range nodeClients {
		if nodeUsage[nc.nodeUUID] == nil {
			var count int64
			nodeUsage[nc.nodeUUID] = &count
		}
	}
	nodeUsageMu.Unlock()

	log.Printf("âœ… å·²é¢„åŠ è½½ %d ä¸ªèŠ‚ç‚¹çš„å®¢æˆ·ç«¯å¼•ç”¨ï¼Œå‡å°‘é”ç«äº‰", len(nodeClients))

	// åˆ›å»ºä»»åŠ¡é€šé“å’Œå·¥ä½œ goroutine
	// ä½¿ç”¨ç¼“å†²é€šé“ï¼Œå®¹é‡è®¾ä¸ºä»»åŠ¡æ•°çš„2å€ï¼Œä»¥ä¾¿é‡è¯•ä»»åŠ¡å¯ä»¥é‡æ–°å…¥é˜Ÿ
	taskChan := make(chan int, repeatCount*2)
	var wg sync.WaitGroup
	// ä¼˜åŒ–ï¼šä½¿ç”¨ sync.Map å‡å°‘é”ç«äº‰ï¼ˆå¹¶å‘å®‰å…¨çš„mapï¼‰
	taskCompleted := &sync.Map{} // map[int]bool

	// å¯åŠ¨å·¥ä½œ goroutine
	for i := 0; i < concurrency; i++ {
		wg.Add(1)
		go func(workerID int) {
			defer wg.Done()

			for taskID := range taskChan {
				// ä¼˜åŒ–ï¼šä½¿ç”¨ sync.Map å¿«é€Ÿæ£€æŸ¥ä»»åŠ¡æ˜¯å¦å·²å®Œæˆï¼ˆæ— é”è¯»å–ï¼‰
				if _, completed := taskCompleted.Load(taskID); completed {
					continue // ä»»åŠ¡å·²å®Œæˆï¼Œè·³è¿‡
				}

				// ä¼˜åŒ–ï¼šä½¿ç”¨å…¨å±€åŸå­è®¡æ•°å™¨è¿›è¡ŒçœŸæ­£çš„è½®è¯¢ï¼ˆæ‰€æœ‰workerå…±äº«ï¼‰
				nodeIndex := atomic.AddInt64(&globalNodeIndex, 1)
				selectedNode := nodeClients[int(nodeIndex-1)%len(nodeClients)]

				// è®°å½•èŠ‚ç‚¹ä½¿ç”¨ï¼ˆä½¿ç”¨åŸå­æ“ä½œï¼Œæ— éœ€åŠ é”ï¼Œå› ä¸ºå·²ç»é¢„å…ˆåˆå§‹åŒ–ï¼‰
				countPtr := nodeUsage[selectedNode.nodeUUID]
				atomic.AddInt64(countPtr, 1)

				var resp *tasksmanager.TaskResponse
				var err2 error
				var protocolUsed string
				startTime := time.Now()

				// å¦‚æœæ˜¯ both æ¨¡å¼ï¼Œä¼˜å…ˆä½¿ç”¨ TUIC åè®®
				if protocolType == "both" && selectedNode.hasTUIC && selectedNode.tuicClient != nil {
					// ç›´æ¥ä½¿ç”¨é¢„åŠ è½½çš„ TUIC å®¢æˆ·ç«¯ï¼ˆæ— é”ï¼Œå¤ç”¨å·²å»ºç«‹çš„è¿æ¥ï¼‰
					resp, err2 = selectedNode.tuicClient.SubmitTask(ctx, req)
					protocolUsed = "TUIC"

					// å¦‚æœ TUIC å¤±è´¥ï¼Œå›é€€åˆ° gRPC
					if err2 != nil {
						protocolUsed = "gRPC"
						resp, err2 = selectedNode.grpcClient.SubmitTask(ctx, req)
					}
				} else {
					// ä½¿ç”¨ gRPCï¼ˆæ— é”ï¼Œä½¿ç”¨é¢„åŠ è½½çš„å®¢æˆ·ç«¯ï¼‰
					protocolUsed = "gRPC"
					resp, err2 = selectedNode.grpcClient.SubmitTask(ctx, req)
				}

				elapsed := time.Since(startTime)

				if err2 != nil {
					// ç½‘ç»œé”™è¯¯ï¼šé‡æ–°æ”¾å…¥ä»»åŠ¡æ± ï¼Œè®©å…¶ä»–èŠ‚ç‚¹é‡è¯•
					// å…ˆæ£€æŸ¥ä»»åŠ¡æ˜¯å¦å·²å®Œæˆï¼ˆå¯èƒ½å…¶ä»–workerå·²ç»å®Œæˆäº†ï¼‰
					if _, alreadyCompleted := taskCompleted.Load(taskID); !alreadyCompleted {
						nodeAddr := selectedNode.nodeAddr
						if protocolUsed == "TUIC" && selectedNode.tuicAddr != "" {
							nodeAddr = selectedNode.tuicAddr
						}
						log.Printf("âš ï¸ [Worker %d] è¯·æ±‚ #%d å¤±è´¥ï¼Œé‡æ–°æ”¾å…¥ä»»åŠ¡æ±  (èŠ‚ç‚¹: %s, åè®®: %s): %v", workerID, taskID+1, nodeAddr, protocolUsed, err2)

						// é‡æ–°æ”¾å…¥ä»»åŠ¡æ± ï¼Œè®©å…¶ä»–èŠ‚ç‚¹é‡è¯•
						select {
						case taskChan <- taskID:
							// æˆåŠŸé‡æ–°å…¥é˜Ÿ
						case <-ctx.Done():
							return
						}
					}
					continue
				}

				statusCode := getResponseStatusCode(resp)
				if statusCode != 200 {
					// é200çŠ¶æ€ç ï¼šé‡æ–°æ”¾å…¥ä»»åŠ¡æ± ï¼Œè®©å…¶ä»–èŠ‚ç‚¹é‡è¯•
					// å…ˆæ£€æŸ¥ä»»åŠ¡æ˜¯å¦å·²å®Œæˆï¼ˆå¯èƒ½å…¶ä»–workerå·²ç»å®Œæˆäº†ï¼‰
					if _, alreadyCompleted := taskCompleted.Load(taskID); !alreadyCompleted {
						nodeAddr := selectedNode.nodeAddr
						if protocolUsed == "TUIC" && selectedNode.tuicAddr != "" {
							nodeAddr = selectedNode.tuicAddr
						}
						log.Printf("âš ï¸ [Worker %d] è¯·æ±‚ #%d è¿”å›é 200 çŠ¶æ€ç ï¼Œé‡æ–°æ”¾å…¥ä»»åŠ¡æ±  (èŠ‚ç‚¹: %s, åè®®: %s): %d", workerID, taskID+1, nodeAddr, protocolUsed, statusCode)

						// é‡æ–°æ”¾å…¥ä»»åŠ¡æ± ï¼Œè®©å…¶ä»–èŠ‚ç‚¹é‡è¯•
						select {
						case taskChan <- taskID:
							// æˆåŠŸé‡æ–°å…¥é˜Ÿ
						case <-ctx.Done():
							return
						}
					}
					continue
				}

				// æˆåŠŸå®Œæˆï¼šçŠ¶æ€ç 200
				// ä½¿ç”¨ sync.Map çš„ LoadOrStore åŸå­æ“ä½œï¼Œé¿å…é‡å¤è®¡æ•°
				if _, loaded := taskCompleted.LoadOrStore(taskID, true); !loaded {
					// ç¬¬ä¸€æ¬¡æ ‡è®°ä¸ºå®Œæˆï¼Œå¢åŠ è®¡æ•°
					atomic.AddInt64(&completedTasks, 1)
				}

				// è®°å½•ç¬¬ä¸€æ¬¡è¯·æ±‚çš„æ—¶é—´
				firstRequestOnce.Do(func() {
					firstRequestTime = elapsed
				})

				// ä¼˜åŒ–ï¼šåªé‡‡æ ·éƒ¨åˆ†è¯·æ±‚çš„è€—æ—¶ï¼ˆæ¯10ä¸ªé‡‡æ ·1ä¸ªï¼‰ï¼Œå‡å°‘å†…å­˜å’Œé”å¼€é”€
				if (taskID+1)%10 == 0 {
					requestTimesMu.Lock()
					requestTimes = append(requestTimes, elapsed)
					requestTimesMu.Unlock()
				}

				// ç»Ÿè®¡å“åº”ä½“å¤§å°
				if resp.TaskResponseBody != nil {
					atomic.AddInt64(&totalBytes, int64(len(resp.TaskResponseBody)))
				}

				// å‡å°‘æ—¥å¿—è¾“å‡ºé¢‘ç‡ï¼šæ¯100ä¸ªè¯·æ±‚è¾“å‡ºä¸€æ¬¡è¿›åº¦ï¼Œé¿å…æ—¥å¿—I/Oé˜»å¡
				if (taskID+1)%100 == 0 || taskID == 0 {
					nodeAddr := selectedNode.nodeAddr
					if protocolUsed == "TUIC" && selectedNode.tuicAddr != "" {
						nodeAddr = selectedNode.tuicAddr
					}
					log.Printf("âœ… [Worker %d] è¯·æ±‚ #%d (èŠ‚ç‚¹: %s, åè®®: %s): çŠ¶æ€ç =%d, è€—æ—¶=%v, å“åº”å¤§å°=%d å­—èŠ‚",
						workerID, taskID+1, nodeAddr, protocolUsed, statusCode, elapsed, getResponseBodySize(resp))
				}
			}
		}(i)
	}

	// å‘é€æ‰€æœ‰åˆå§‹ä»»åŠ¡
	for i := 0; i < repeatCount; i++ {
		taskChan <- i
	}

	// ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼ˆåŒ…æ‹¬é‡è¯•çš„ä»»åŠ¡ï¼‰
	// ä½¿ç”¨ä¸€ä¸ªå•ç‹¬çš„goroutineæ¥ç›‘æ§ä»»åŠ¡å®Œæˆæƒ…å†µ
	done := make(chan struct{})
	go func() {
		ticker := time.NewTicker(100 * time.Millisecond) // å‡å°‘æ£€æŸ¥é—´éš”ï¼Œæ›´å¿«å“åº”
		defer ticker.Stop()
		for {
			select {
			case <-ticker.C:
				completed := atomic.LoadInt64(&completedTasks)
				if completed >= int64(repeatCount) {
					// æ‰€æœ‰ä»»åŠ¡éƒ½å·²å®Œæˆï¼Œå…³é—­é€šé“è®©workeré€€å‡º
					close(taskChan)
					close(done)
					return
				}
			case <-ctx.Done():
				close(taskChan)
				close(done)
				return
			}
		}
	}()

	// ç­‰å¾…æ‰€æœ‰ goroutine å®Œæˆ
	wg.Wait()
	<-done // ç¡®ä¿ç›‘æ§goroutineä¹Ÿå®Œæˆ

	totalElapsed := time.Since(totalStartTime)
	completed := atomic.LoadInt64(&completedTasks)
	failed := atomic.LoadInt64(&failedTasks)
	totalBytesCount := atomic.LoadInt64(&totalBytes)

	// è®¡ç®—ç»Ÿè®¡ä¿¡æ¯ï¼ˆä¸ submitRealTaskMultipleTimes ç›¸åŒï¼‰
	var (
		avgTime    time.Duration
		minTime    time.Duration
		maxTime    time.Duration
		medianTime time.Duration
	)

	if len(requestTimes) > 0 {
		var sum time.Duration
		minTime = requestTimes[0]
		maxTime = requestTimes[0]

		for _, t := range requestTimes {
			sum += t
			if t < minTime {
				minTime = t
			}
			if t > maxTime {
				maxTime = t
			}
		}
		avgTime = sum / time.Duration(len(requestTimes))

		// ä¼˜åŒ–ï¼šä½¿ç”¨æ ‡å‡†åº“çš„å¿«é€Ÿæ’åºï¼ˆO(n log n)ï¼‰ï¼Œè€Œä¸æ˜¯O(nÂ²)çš„å†’æ³¡æ’åº
		sortedTimes := make([]time.Duration, len(requestTimes))
		copy(sortedTimes, requestTimes)
		if len(sortedTimes) > 0 {
			sort.Slice(sortedTimes, func(i, j int) bool {
				return sortedTimes[i] < sortedTimes[j]
			})
			medianTime = sortedTimes[len(sortedTimes)/2]
		}
	}

	// è¾“å‡ºç»Ÿè®¡ç»“æœ
	log.Println()
	log.Println("=" + strings.Repeat("=", 60))
	log.Println("=== æ‰¹é‡è¯·æ±‚æ€§èƒ½ç»Ÿè®¡ï¼ˆèŠ‚ç‚¹æ± è´Ÿè½½å‡è¡¡ï¼‰===")
	log.Printf("æ€»è¯·æ±‚æ•°: %d", repeatCount)
	log.Printf("æˆåŠŸ: %d", completed)
	log.Printf("å¤±è´¥: %d", failed)
	log.Printf("æ€»è€—æ—¶: %v", totalElapsed)
	log.Printf("å¹³å‡ QPS: %.2f è¯·æ±‚/ç§’", float64(completed)/totalElapsed.Seconds())
	log.Printf("æ€»ä¼ è¾“æ•°æ®: %.2f KB (%.2f MB)", float64(totalBytesCount)/1024, float64(totalBytesCount)/1024/1024)
	log.Println()
	log.Println("--- è¯·æ±‚è€—æ—¶ç»Ÿè®¡ ---")
	if firstRequestTime > 0 {
		log.Printf("é¦–æ¬¡è¯·æ±‚è€—æ—¶: %v", firstRequestTime)
	}
	if avgTime > 0 {
		log.Printf("å¹³å‡è€—æ—¶: %v", avgTime)
	}
	if minTime > 0 {
		log.Printf("æœ€å¿«è¯·æ±‚: %v", minTime)
	}
	if maxTime > 0 {
		log.Printf("æœ€æ…¢è¯·æ±‚: %v", maxTime)
	}
	if medianTime > 0 {
		log.Printf("ä¸­ä½æ•°è€—æ—¶: %v", medianTime)
	}

	// è¾“å‡ºèŠ‚ç‚¹ä½¿ç”¨ç»Ÿè®¡
	log.Println()
	log.Println("--- èŠ‚ç‚¹ä½¿ç”¨ç»Ÿè®¡ï¼ˆè´Ÿè½½å‡è¡¡æ•ˆæœï¼‰---")
	nodeUsageMu.Lock()
	for _, nc := range nodeClients {
		if countPtr := nodeUsage[nc.nodeUUID]; countPtr != nil {
			count := atomic.LoadInt64(countPtr)
			if completed > 0 {
				percentage := float64(count) / float64(completed) * 100
				log.Printf("èŠ‚ç‚¹ %s: %d æ¬¡è¯·æ±‚ (%.1f%%)", nc.nodeUUID, count, percentage)
			}
		}
	}
	nodeUsageMu.Unlock()

	log.Println("=" + strings.Repeat("=", 60))

	return nil
}
